{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710701891957,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"WNuEkOTbpIv7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/mayank_khulbe_farmart_co/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/home/mayank_khulbe_farmart_co/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n","  return torch._C._cuda_getDeviceCount() > 0\n"]}],"source":["import torch\n","from torch import nn\n","from torchvision import datasets, transforms\n","import math\n","import time\n","import logging\n","import matplotlib.pyplot as plt\n","import itertools\n","import numpy as np\n","from tqdm import tqdm\n","import torchvision.utils as vutils\n","import os\n","import textwrap\n","import torch.optim as optim\n","\n","#optuna\n","import optuna\n","from optuna.trial import TrialState\n","from optuna.artifacts import FileSystemArtifactStore\n","from optuna.artifacts import upload_artifact\n","\n","#optuna dashboard packages\n","from optuna_dashboard import save_note, register_objective_form_widgets, ChoiceWidget\n","from optuna_dashboard.artifact import get_artifact_path\n","\n","torch.manual_seed(111)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1710700004649,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"OxTT99CAqWo2"},"outputs":[],"source":["def get_mnist_loaders(train_batch_size, test_batch_size):\n","    \"\"\"Get MNIST data loaders\"\"\"\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=True, download=True,\n","                       transform=transforms.Compose([\n","                           transforms.ToTensor(),\n","                           transforms.Normalize((0.5,), (0.5,))\n","                       ])),\n","        batch_size=train_batch_size, shuffle=True)\n","\n","    test_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                           transforms.ToTensor(),\n","                           transforms.Normalize((0.5,), (0.5,))\n","                       ])),\n","        batch_size=test_batch_size, shuffle=True)\n","\n","    return train_loader, test_loader"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1710700011201,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"HXhGsI5oqjZn"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(784, 1024),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(1024, 512),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), 784)\n","        output = self.model(x)\n","        return output"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1710700017246,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"UBjrZAGKqpj1"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(128, 256),\n","            nn.LeakyReLU(),\n","            nn.Linear(256, 512),\n","            nn.LeakyReLU(),\n","            nn.Linear(512, 1024),\n","            nn.LeakyReLU(),\n","            nn.Linear(1024, 784),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        output = self.model(x)\n","        output = output.view(x.size(0), 1, 28, 28)\n","        return output"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1710700078420,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"ed9kFcYYqrBq"},"outputs":[],"source":["def train_discriminator(discriminator, images, real_labels, fake_images, fake_labels, criterion, d_optimizer):\n","    discriminator.zero_grad()\n","    outputs = discriminator(images)\n","    real_loss = criterion(outputs, real_labels.unsqueeze(1))\n","    real_score = outputs\n","\n","    outputs = discriminator(fake_images)\n","    fake_loss = criterion(outputs, fake_labels.unsqueeze(1))\n","    fake_score = outputs\n","\n","    d_loss = real_loss + fake_loss\n","    d_loss.backward()\n","\n","    d_optimizer.step()\n","    return d_loss, real_score, fake_score"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1710700080026,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"q2ARWDyUqs1w"},"outputs":[],"source":["def train_generator(generator, discriminator_outputs, real_labels, criterion, g_optimizer):\n","    generator.zero_grad()\n","    g_loss = criterion(discriminator_outputs, real_labels.unsqueeze(1))\n","    g_loss.backward()\n","\n","    g_optimizer.step()\n","    return g_loss"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1710701327946,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"yHfHu8bWsbzA"},"outputs":[],"source":["# Plot grid of 9 images from generator after each epoch\n","def generate_new_images(generator, sample_images, latent_dim, img_dir):\n","    fixed_noise = torch.randn(sample_images, latent_dim).to(device)  # Sample 15 images\n","    fake_images = generator(fixed_noise).to(device)\n","\n","    plt.figure(figsize=(5, 5))\n","    plt.axis(\"off\")\n","    plt.title(\"Generated Images\")\n","    plt.imshow(\n","        np.transpose(\n","            vutils.make_grid(fake_images, nrow=5, padding=1, normalize=True).cpu().numpy(),\n","            (1, 2, 0)\n","        )\n","    )\n","    plt.savefig(img_dir)\n","    plt.show()\n","    plt.close()"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":603,"status":"ok","timestamp":1710701850966,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"I25rgt_pquL1"},"outputs":[],"source":["def train_GANs(study: optuna.Study,\n","               artifact_store: FileSystemArtifactStore):\n","\n","    trial = study.ask() #start a trial\n","\n","    print(f\"running trial number: {trial.number}\")\n","\n","    latent_dim = 128\n","\n","    #define the generator and the discriminator\n","    discriminator = Discriminator().to(device=device)\n","    generator = Generator().to(device=device)\n","\n","    cfg = {\n","        \"train_batch_size\": trial.suggest_categorical(\"train_batch_size\", [64, 128]),\n","        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","        \"num_epochs\": 100,\n","        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n","        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\"])\n","    }\n","\n","    #define the loader\n","    batch_size = cfg[\"train_batch_size\"]\n","    train_loader, _ = get_mnist_loaders(batch_size, batch_size)\n","\n","\n","    #define the optimizers\n","    lr = cfg['lr']\n","    optimizer_name = cfg['optimizer']\n","    d_optimizer = getattr(optim, optimizer_name)(discriminator.parameters(), lr=lr)  # Instantiate optimizer from name\n","    g_optimizer = getattr(optim, optimizer_name)(generator.parameters(), lr=lr)  # Instantiate optimizer from name\n","\n","    #define the criterion\n","    criterion = nn.BCELoss()\n","\n","    print(f\"Batch Size: {batch_size}\\nLearning Rate: {lr}\\nOptimizer: {optimizer_name}\")\n","\n","    for epoch in range(cfg['num_epochs']):\n","\n","        print(f\"running epoch number: {epoch}\")\n","\n","        for n, (images, _) in tqdm(enumerate(train_loader)):\n","            images = images.to(device)\n","            real_labels = torch.ones(images.size(0)).to(device)\n","\n","            noise = torch.randn(images.size(0), latent_dim).to(device)\n","            fake_images = generator(noise)\n","            fake_labels = torch.zeros(images.size(0)).to(device)\n","\n","            # Train the discriminator\n","            d_loss, real_score, fake_score = train_discriminator(discriminator, images,\n","                                                                 real_labels, fake_images, fake_labels,\n","                                                                  criterion, d_optimizer)\n","\n","            noise = torch.randn(images.size(0), latent_dim).to(device)\n","            fake_images = generator(noise)\n","            outputs = discriminator(fake_images)\n","\n","            # Train the generator\n","            g_loss = train_generator(generator, outputs, real_labels, criterion, g_optimizer)\n","\n","            if (n+1) % len(train_loader) == 0:\n","\n","                print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f, '\n","                    'D(x): %.2f, D(G(z)): %.2f'\n","                    % (epoch + 1, cfg['num_epochs'], n + 1, len(train_loader), d_loss.item(), g_loss.item(),\n","                        real_score.mean().item(), fake_score.mean().item()))\n","\n","    img_path = f\"tmp/generated_image-{trial.number}.png\"\n","    generate_new_images(generator, 30, latent_dim, img_path)\n","\n","    artifacts_id = upload_artifact(trial, img_path, artifact_store)\n","    artifact_path = get_artifact_path(trial, artifacts_id)\n","\n","    # 4. Save Note\n","    note = textwrap.dedent(\n","        f\"\"\"\\\n","    ## Trial {trial.number}\n","\n","    Grid of GAN generated images!!\n","    ![generated-images]({artifact_path})\n","\n","    d_loss: {d_loss.item():.2f}\\n g_loss: {g_loss.item():.2f}\n","    \"\"\"\n","    )\n","    save_note(trial, note)\n","\n","    return g_loss.item(), d_loss.item()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":635,"status":"ok","timestamp":1710701904370,"user":{"displayName":"mayank khulbe","userId":"08775446541722101300"},"user_tz":-330},"id":"nqi0naIIxA5p"},"outputs":[],"source":["def start_optimization(artifact_store: FileSystemArtifactStore):\n","    # 1. Create Study\n","    storage = \"sqlite:///db.sqlite3\"\n","    study = optuna.create_study(study_name=\"HITL_with_optuna_for_digit_generation\",\n","                                directions=['minimize', 'maximize'],\n","                                storage=storage,\n","                                load_if_exists=True)\n","\n","    # 2. Set an objective name\n","    study.set_metric_names([\"Are you satisfied with the model's generated images?\", \"Are you satisfied with the discriminator's performance?\"])\n","\n","    # 3. Register ChoiceWidget\n","    register_objective_form_widgets(\n","    study,\n","    widgets=[\n","        ChoiceWidget(\n","            choices=[\"Yes 👍\", \"Somewhat 👌\", \"No 👎\"],\n","            values=[-1, 0, 1],\n","            description=\"Please input your score for generated images!\",\n","        ),\n","        ChoiceWidget(\n","            choices=[\"Yes 👍\", \"Somewhat 👌\", \"No 👎\"],\n","            values=[1, 0, -1],\n","            description=\"Please input your score for model performance!\",\n","        ),\n","    ],\n",")\n","\n","    # 4. Start Human-in-the-loop Optimization\n","    n_batch = 4\n","    while True:\n","        running_trials = study.get_trials(deepcopy=False, states=(TrialState.RUNNING,))\n","        if len(running_trials) >= n_batch:\n","            time.sleep(1)  # Avoid busy-loop\n","            continue\n","        train_GANs(study, artifact_store)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hfgHFBIHx-tI"},"outputs":[],"source":["def main():\n","    # tmp_path = os.path.join(os.path.dirname(__file__), \"tmp\")\n","    # Get the absolute path to the current notebook file\n","    notebook_dir = os.getcwd()\n","\n","    # Create the absolute path to the \"tmp\" folder\n","    tmp_path = os.path.join(notebook_dir, \"tmp\")\n","\n","    # 1. Create Artifact Store\n","    # artifact_path = os.path.join(os.path.dirname(__file__), \"artifact\")\n","    artifact_path = os.path.join(notebook_dir, \"artifact\")\n","    artifact_store = FileSystemArtifactStore(artifact_path)\n","\n","    print(f\"paths : {tmp_path}, {artifact_path}\")\n","\n","    if not os.path.exists(artifact_path):\n","        os.mkdir(artifact_path)\n","\n","    if not os.path.exists(tmp_path):\n","        os.mkdir(tmp_path)\n","\n","    # 2. Run optimize loop\n","    start_optimization(artifact_store)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/tmp/ipykernel_213863/3673421781.py:12: ExperimentalWarning: FileSystemArtifactStore is experimental (supported from v3.3.0). The interface can change in the future.\n","  artifact_store = FileSystemArtifactStore(artifact_path)\n","[I 2024-03-25 04:53:58,623] Using an existing study with name 'HITL_with_optuna_for_digit_generation' instead of creating a new one.\n","/var/tmp/ipykernel_213863/2331763268.py:10: ExperimentalWarning: set_metric_names is experimental (supported from v3.2.0). The interface can change in the future.\n","  study.set_metric_names([\"Are you satisfied with the model's generated images?\", \"Are you satisfied with the discriminator's performance?\"])\n"]},{"name":"stdout","output_type":"stream","text":["paths : /home/mayank_khulbe_farmart_co/fmt/Optuna/experiments/tmp, /home/mayank_khulbe_farmart_co/fmt/Optuna/experiments/artifact\n","running trial number: 2\n","Batch Size: 64\n","Learning Rate: 2.7696019722038554e-05\n","Optimizer: Adam\n","running epoch number: 0\n"]},{"name":"stderr","output_type":"stream","text":["130it [00:05, 22.95it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     main()\n","Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     os\u001b[39m.\u001b[39mmkdir(tmp_path)\n\u001b[1;32m     22\u001b[0m \u001b[39m# 2. Run optimize loop\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m start_optimization(artifact_store)\n","Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36mstart_optimization\u001b[0;34m(artifact_store)\u001b[0m\n\u001b[1;32m     34\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)  \u001b[39m# Avoid busy-loop\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m train_GANs(study, artifact_store)\n","Cell \u001b[0;32mIn[8], line 42\u001b[0m, in \u001b[0;36mtrain_GANs\u001b[0;34m(study, artifact_store)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(cfg[\u001b[39m'\u001b[39m\u001b[39mnum_epochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrunning epoch number: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mfor\u001b[39;00m n, (images, _) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_loader)):\n\u001b[1;32m     43\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m         real_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mto(device)\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:349\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/transforms/_functional_tensor.py:926\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    925\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 926\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39;49mdiv_(std)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOFaTqsdQJ8mx9RPBdK0fmu","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"optuna","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"vscode":{"interpreter":{"hash":"11ac6faa9248008d391a7540e44edcb95f152bb1f0fd3624523abee6a1a360c7"}}},"nbformat":4,"nbformat_minor":0}
